{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Logistic Regression\n",
    "\n",
    " ---------------\n",
    "\n",
    " Adapted from Columbia Engineering | Emeritus course Applied Machine Learning:\n",
    "_Authors: Khal Makhoul, W.P.G.Peterson_\n",
    "\n",
    " ## Project Guide\n",
    " ------------\n",
    " - [Project Overview](#overview)\n",
    " - [Introduction and Review](#intro)\n",
    " - [Data Exploration](#data)\n",
    " - [Coding Logistic Regression](#code)\n",
    " - [Logistic Regression in `sklearn`](#sklearn)\n",
    "\n",
    " <a id = \"overview\"></a>\n",
    " ## Project Overview\n",
    " -------------\n",
    "\n",
    " This project works through the definition of a Logistic Regression function in `Python`. After a summary of the equations that will be used, and a brief EDA of the \"Titanic\" data used, we define a number of functions which create a Logistic Regression.\n",
    " A demonstration of `sklearn`'s implementation of Logistic Regression also follows.\n",
    "\n",
    " We code functions to do the following:\n",
    " 1. Implement the Logistic Regression Algorithm\n",
    "     - Calculate the value of the sigmoid function\n",
    "     - Calculate the gradient of the log-likelihood with respect to $w$\n",
    "     - Sum the gradients of the log-likelihood with respect to $w$\n",
    " 2. Execute logistic regression, stopping after a particular iteration\n",
    " 3. Determine convergence of the logistic regression algorithm\n",
    "\n",
    " **Motivation**: Logistic Regression offers a way to to create a fairly interpretable parametic model for binary classification.\n",
    "\n",
    " **Problem**: Using Logistic Regression, predict whether or not a passenger survived the sinking of the Titanic.\n",
    "\n",
    " **Data**: The data comes from [Kaggle's Titanic Data](https://www.kaggle.com/c/titanic/data)\n",
    "\n",
    " Please see above link for a more complete description of the data.\n",
    "\n",
    " <a id = \"intro\"></a>\n",
    "\n",
    " ### Introduction and Review\n",
    "\n",
    " Recall that the likelihood for Logistic Regression is given by:\n",
    "\n",
    " $$p(y_1,\\ ...,\\ y_n\\ |\\ x_1,\\ ...,\\ x_n,\\ w)\\ =\\prod\\limits_{i=1}^n\\ \\sigma_i(y_i \\cdot w)$$\n",
    "\n",
    " For coding purposes, we need the expression for the gradient of the log-likelihood with respect to $w$:\n",
    "\n",
    "\n",
    " $$\\nabla_w \\mathcal{L} = \\sum_{i = 1}^n (1 − \\sigma_i(y_i \\cdot w))\\ y_i x_i$$\n",
    "\n",
    " Where: $$\\sigma_i(y_i \\cdot w) = \\frac{e^{y_iX_i^Tw}}{1+e^{y_ix_i^Tw}}$$\n",
    "\n",
    "\n",
    " <a id = \"data\"></a>\n",
    " ### Data Exploration\n",
    "\n",
    " This project analyzes data from the Titanic passenger manifest. Demographic and trip information for each passenger is coupled with whether or not they survived the disaster.\n",
    "\n",
    " We start by examining the data as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                   Moran, Mr. James    male   NaN      0   \n",
       "6                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  \n",
       "5      0            330877   8.4583   NaN        Q  \n",
       "6      0             17463  51.8625   E46        S  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary modules and sets a few plotting parameters for display\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "\n",
    "# Load the data into a `pandas` DataFrame object\n",
    "tr_path = './train.csv'\n",
    "titanic_df = pd.read_csv(tr_path)\n",
    "\n",
    "# Examine head of df\n",
    "titanic_df.head(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Dropping nulls.\n",
    "\n",
    " The exercise below requires dropping certain records / columns.\n",
    "\n",
    " The general rules followed below are:\n",
    "\n",
    " - If a column consists mostly of missing data, that column probably will not be of much use in prediction.\n",
    " - If a column has very few missing values, and enough records to build a model are complete, the records with missing values in that column may be cast out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n",
      "PassengerId    0.000000\n",
      "Survived       0.000000\n",
      "Pclass         0.000000\n",
      "Name           0.000000\n",
      "Sex            0.000000\n",
      "Age            0.198653\n",
      "SibSp          0.000000\n",
      "Parch          0.000000\n",
      "Ticket         0.000000\n",
      "Fare           0.000000\n",
      "Cabin          0.771044\n",
      "Embarked       0.002245\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### 1. Drop all of the columns in `titanic_df` which are filled more than 50% with nulls.\n",
    "### 2. If a column has fewer than 10 missing values:\n",
    "### ### Drop all of the records with missing data in that column.\n",
    "\n",
    "print(titanic_df.shape)\n",
    "print(titanic_df.isna().sum()/titanic_df.shape[0])\n",
    "#Column Cabin is 77% (>50%) na, drop it\n",
    "#Column Embarked  2 (<10) na, drop it\n",
    "\n",
    "titanic_clean_df = titanic_df.drop(['Cabin'], axis=1)\n",
    "titanic_clean_df.dropna(subset=['Embarked'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Given the fairly large number of values missing from \"Age\", and the feature's likely relationship with survival, we will create an educated guess for missing passenger ages; imputing the ages using a $k$-Nearest-Neighbor algorithm.\n",
    "\n",
    " Note: In imputing values for \"age\",  \"`survival`\" will be excluded from the $X$ matrix since that is the value we ultimately plan to predict.\n",
    "\n",
    " #### KNeighborsRegressor in `sklearn`\n",
    " Because `sklearn` automatically converts all data to floats before fitting models, it is necessary to encode any and all categorical variables as dummy variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0       3    male  22.0      1      0   7.2500        S\n",
       "1       1  female  38.0      1      0  71.2833        C\n",
       "2       3  female  26.0      0      0   7.9250        S\n",
       "3       1  female  35.0      1      0  53.1000        S\n",
       "4       3    male  35.0      0      0   8.0500        S"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Drop irrelevant categories\n",
    "titanic_df.drop(['Ticket','Cabin', 'PassengerId', 'Name'], axis=1, inplace=True)\n",
    "titanic_df = titanic_df.loc[titanic_df['Embarked'].notnull(),:]\n",
    "\n",
    "### Drop \"Survived\" for purposes of KNN imputation:\n",
    "y_target = titanic_df.Survived\n",
    "titanic_knn = titanic_df.drop(['Survived'], axis = 1)  \n",
    "titanic_knn.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass   Age  SibSp  Parch     Fare  Sex_male  Embarked_Q  Embarked_S\n",
       "0       3  22.0      1      0   7.2500         1           0           1\n",
       "1       1  38.0      1      0  71.2833         0           0           0\n",
       "2       3  26.0      0      0   7.9250         0           0           1\n",
       "3       1  35.0      1      0  53.1000         0           0           1\n",
       "4       3  35.0      0      0   8.0500         1           0           1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Adding dummy variables for categorical vars\n",
    "to_dummy = ['Sex','Embarked']\n",
    "titanic_knn = pd.get_dummies(titanic_knn, prefix = to_dummy, columns = to_dummy, drop_first = True)\n",
    "\n",
    "titanic_knn.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data to Impute\n",
      "    Pclass  SibSp  Parch     Fare  Sex_male  Embarked_Q  Embarked_S\n",
      "5        3      0      0   8.4583         1           1           0\n",
      "17       2      0      0  13.0000         1           0           1\n",
      "19       3      0      0   7.2250         0           0           0\n",
      "\n",
      "Imputed Ages\n",
      "    Pclass  SibSp  Parch     Fare  Sex_male  Embarked_Q  Embarked_S   Age\n",
      "5        3      0      0   8.4583         1           1           0  47.2\n",
      "17       2      0      0  13.0000         1           0           1  25.6\n",
      "19       3      0      0   7.2250         0           0           0  23.0\n",
      "Shape with imputed values: (889, 8)\n",
      "Shape before imputation: (889, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>47.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass   Age  SibSp  Parch     Fare  Sex_male  Embarked_Q  Embarked_S\n",
       "0       3  22.0      1      0   7.2500         1           0           1\n",
       "1       1  38.0      1      0  71.2833         0           0           0\n",
       "2       3  26.0      0      0   7.9250         0           0           1\n",
       "3       1  35.0      1      0  53.1000         0           0           1\n",
       "4       3  35.0      0      0   8.0500         1           0           1\n",
       "5       3  47.2      0      0   8.4583         1           1           0\n",
       "6       1  54.0      0      0  51.8625         1           0           1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Splitting data - on whether or not \"Age\" is specified.\n",
    "\n",
    "# Training data -- \"Age\" Not null; \"Age\" as target\n",
    "train = titanic_knn[titanic_knn.Age.notnull()]\n",
    "X_train = train.drop(['Age'], axis = 1)\n",
    "y_train = train.Age\n",
    "\n",
    "\n",
    "# Data to impute, -- Where Age is null; Remove completely-null \"Age\" column.\n",
    "impute = titanic_knn[titanic_knn.Age.isnull()].drop(['Age'], axis = 1)\n",
    "print(\"Data to Impute\")\n",
    "print(impute.head(3))\n",
    "\n",
    "# import algorithm\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Instantiate\n",
    "knr = KNeighborsRegressor()\n",
    "\n",
    "# Fit\n",
    "knr.fit(X_train, y_train)\n",
    "\n",
    "# Create Predictions\n",
    "imputed_ages = knr.predict(impute)\n",
    "\n",
    "# Add to Df\n",
    "impute['Age'] = imputed_ages\n",
    "print(\"\\nImputed Ages\")\n",
    "print(impute.head(3))\n",
    "\n",
    "# Re-combine dataframes\n",
    "titanic_imputed = pd.concat([train, impute], sort = False, axis = 0)\n",
    "\n",
    "# Return to original order - to match back up with \"Survived\"\n",
    "titanic_imputed.sort_index(inplace = True)\n",
    "print(\"Shape with imputed values:\", titanic_imputed.shape)\n",
    "print(\"Shape before imputation:\", titanic_knn.shape)\n",
    "titanic_imputed.head(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It would be appropriate to spend more time taking care with the imputation of age. For brevity's sake that will not be done here.\n",
    "\n",
    " #### Brief EDA\n",
    "\n",
    " First Look at tabulations of categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Combos or categorical vars: \n",
      " [('Pclass', 'Sex'), ('Pclass', 'Embarked'), ('Sex', 'Embarked')] \n",
      "\n",
      "Row Percents: \n",
      " Sex       female      male\n",
      "Pclass                    \n",
      "1       0.429907  0.570093\n",
      "2       0.413043  0.586957\n",
      "3       0.293279  0.706721 \n",
      "\n",
      "Column Percents: \n",
      " Sex       female      male\n",
      "Pclass                    \n",
      "1       0.294872  0.211438\n",
      "2       0.243590  0.187175\n",
      "3       0.461538  0.601386 \n",
      "---------------\n",
      "\n",
      "Row Percents: \n",
      " Embarked         C         Q         S\n",
      "Pclass                                \n",
      "1         0.397196  0.009346  0.593458\n",
      "2         0.092391  0.016304  0.891304\n",
      "3         0.134420  0.146640  0.718941 \n",
      "\n",
      "Column Percents: \n",
      " Embarked         C         Q         S\n",
      "Pclass                                \n",
      "1         0.505952  0.025974  0.197205\n",
      "2         0.101190  0.038961  0.254658\n",
      "3         0.392857  0.935065  0.548137 \n",
      "---------------\n",
      "\n",
      "Row Percents: \n",
      " Embarked         C         Q         S\n",
      "Sex                                   \n",
      "female    0.233974  0.115385  0.650641\n",
      "male      0.164645  0.071057  0.764298 \n",
      "\n",
      "Column Percents: \n",
      " Embarked         C         Q         S\n",
      "Sex                                   \n",
      "female    0.434524  0.467532  0.315217\n",
      "male      0.565476  0.532468  0.684783 \n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "# Lists of categorical v. numeric features\n",
    "categorical = ['Pclass','Sex','Embarked']\n",
    "numeric = ['Age','SibSp','Parch','Fare']\n",
    "\n",
    "# Create all pairs of categorical variables, look at distributions\n",
    "cat_combos = list(itertools.combinations(categorical, 2))\n",
    "print(\"All Combos or categorical vars: \\n\",cat_combos, \"\\n\")\n",
    "for row, col in cat_combos:\n",
    "    print(\"Row Percents: \\n\",pd.crosstab(titanic_df[row], titanic_df[col], normalize=\"index\"), \"\\n\")\n",
    "    print(\"Column Percents: \\n\", pd.crosstab(titanic_df[row], titanic_df[col], normalize=\"columns\"),\"\\n---------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Correlation heatmap of the numberic variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBQAAAJDCAYAAAC7REfpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xu0bWld3+nPPgUFyEXuWBY3bYsC1BYQC2i65S4YI0UEKYhRSKAr9IBoy0hGYMQBBEOLdhsTDBpLISIxgkjQaiWWyEXtVqgqoMJN7gyhGhRjIcqdqr36jzlPanM479lr1jl777XrPM8Yc5w555rrrHdMWLXP/s3v+3u3VqtVAAAAAEscOegBAAAAAIePggIAAACwmIICAAAAsJiCAgAAALCYggIAAACwmIICAAAAsJiCAgAAABwOL60+Wb1r8PpW9aLqg9U7qvvseO1J1Qfm7UmnYjAKCgAAAHA4/HL1qBO8/t3VOfN2YfXz8/lbV8+t7ledN+/f6mQHo6AAAAAAh8MfVled4PXzq1+pVtWbq1tWZ1WPrF43v/dT8/6JChNrUVAAAACA64ezq4/tOL5yPjc6f1JucLJ/wW5+54bnrvb6M4Cv9hOPuuighwCnpbPPvetBDwFOS9/zvXc56CHAaeuHHtTWQY9hL+3n77Tf8+X3/eOmqQpHXTRv6zre/xarE5w/KXteUAAAAADWsrSAcKwrqzvtOL5j9fH5/IOPOf+mk/icypQHAAAAuL64uPqhpkTC/atPV5+oLqm+q6kR463m/UtO9sMkFAAAAGBg64YbNaPj15qSBrdtSh08t7rh/Nq/r15b/Z2mZSM/V/3D+bWrqh+vLpuPn9+JmzuuRUEBAAAADocn7vL6qnr64LWXztspo6AAAAAAA0dusFEJhY2ihwIAAACwmIQCAAAADGzd0HP4EXcGAAAAWExCAQAAAAb0UBiTUAAAAAAWk1AAAACAga0bSiiMSCgAAAAAi0koAAAAwIAeCmMSCgAAAMBiCgoAAADAYqY8AAAAwICmjGMSCgAAAMBiEgoAAAAwoCnjmIQCAAAAsJiEAgAAAAxsnSGhMCKhAAAAACwmoQAAAAADRyQUhiQUAAAAgMUkFAAAAGBg64iEwoiEAgAAALCYhAIAAAAMbJ3hOfyIOwMAAAAsJqEAAAAAA1Z5GJNQAAAAABaTUAAAAIABqzyMSSgAAAAAiykoAAAAAIuZ8gAAAAADmjKOSSgAAAAAi0koAAAAwMCWhMKQhAIAAACwmIQCAAAADGwd8Rx+xJ0BAAAAFpNQAAAAgIGtI3oojEgoAAAAAItJKAAAAMDAEas8DEkoAAAAAItJKAAAAMCAHgpjEgoAAADAYhIKAAAAMLB1xHP4EXcGAAAAWExCAQAAAAb0UBiTUAAAAAAWU1AAAAAAFjPlAQAAAAaOnGHKw4iEAgAAALCYhAIAAAAMaMo4JqEAAAAALCahAAAAAANbRzyHH3FnAAAAgMUkFAAAAGBAD4UxCQUAAABgMQkFAAAAGJBQGJNQAAAAABaTUAAAAIABCYUxCQUAAABgMQkFAAAAGNg64jn8iDsDAAAALCahAAAAAANHztBDYURCAQAAAFhMQQEAAABYbJ2Cwh2ql1T/ZT6+Z/WUPRsRAAAAbIitI1v7th026xQUfrm6pPr6+fj91f++y3surC6vLr/TUx9/nQcHAAAAbKZ1mjLetvr16tnz8dXVNbu856J562O/9Our6zw6AAAAOECWjRxb5858trpNdbQwcP/q03s2IgAAAGDjrZNQeGZ1cfU/VP9vdbvqcXs5KAAAANgEh7G3wX5Zp6DwtupB1bnVVvW+6st7OSgAAABgs61TUPi+Y47v1jTl4Z3VJ0/5iAAAAGBDSCiMrVNQeEr1gOqN8/GDqzc3FRaeX718T0YGAAAAbKx1Cgrb1T2qv5iP71D9fHW/6g9TUAAAAOB6yioPY+vcmbt2bTGhpmkOd6uuSi8FAAAAOC2tk1D4o+q3q1fNx49tSibctPrrPRoXAAAAHDg9FMbWKSg8vakx4/88H19anVV9tnrIHo0LAAAA2GDrFBRW1YeaeiY8vvpI9eq9HBQAAABsAj0Uxk5UULhb9YTqidVfVa+stpJKAAAAgNPeiQoK723qn/C91Qfncz+65yMCAACATbGlh8LIibIbj63+vHpj9YvVw5oSCgAAAMBp7kQFhddUF1R3r97UlE64Q/Xz1Xft+cgAAACAjbVOd4nPVr9a/d3qjtUV1bP2clAAAACwCbaObO3bdtgsbVd5VfUL1UP3YCwAAADAIbHOspEAAABwWrJs5Jg7AwAAACwmoQAAAAADh7G3wX6RUAAAAAAWk1AAAACAAT0UxtwZAAAAYDEJBQAAABjQQ2FMQgEAAABYTEIBAAAABiQUxiQUAAAAgMUkFAAAAGDEKg9D7gwAAACwmIQCAAAADGxt6aEwIqEAAAAALCahAAAAAANbm9dD4VHVv63OqH6peuExr/9M9ZB5/2uq21e3nI+vqd4573+0evTJDERBAQAAAA6HM6oXV4+orqwuqy6u3rPjmh/dsf9PqnvvOP58da9TNZiNK7UAAAAAx3Ve9cHqw9WXqldU55/g+idWv7ZXg5FQAAAAgIGtIxvVlPHs6mM7jq+s7je49i7VN1Rv2HHuxtXl1dVNUyV+82QGo6AAAAAAm+HCeTvqonk76njVjdXg73pC9RtNfROOunP18eobmwoN76w+dF0Hq6AAAAAAI/vblPHYAsKxrqzutOP4jk0FguN5QvX0Y84dvfbD1Zua+itc54KCHgoAAABwOFxWndM0leHMpqLBxce57tzqVtWf7Dh3q+pG8/5tqwf2lc0cF5NQAAAAgIEN66FwdfWM6pKmFR9eWr27en5Tb4SjxYUnNjVs3Dkd4h7VL1TbTeGCF6agAAAAAKeN187bTs855vh5x3nfH1ffeioHoqAAAAAAA1tbOgWMuDMAAADAYhIKAAAAMLJZPRQ2ioQCAAAAsJiEAgAAAAxsHfEcfsSdAQAAABaTUAAAAICBLT0UhiQUAAAAgMUkFAAAAGBky3P4EXcGAAAAWExBAQAAAFjMlAcAAAAY0JRxTEIBAAAAWExCAQAAAEaOeA4/4s4AAAAAi0koAAAAwMDWlh4KIxIKAAAAwGISCgAAADCih8KQOwMAAAAsJqEAAAAAA1tH9FAYkVAAAAAAFpNQAAAAgJEtz+FH3BkAAABgMQkFAAAAGNFDYUhCAQAAAFhMQgEAAAAGtvRQGHJnAAAAgMX2PKHwE4+6aK8/AjiOZ//uhQc9BDgtnfeTLz/oIcBp6c2fP/ughwCnMcH305X/5QEAAGBEU8YhUx4AAACAxSQUAAAAYGDriOfwI+4MAAAAsJiEAgAAAIxs6aEwIqEAAAAALCahAAAAACN6KAy5MwAAAMBiEgoAAAAwoofCkIQCAAAAsJiEAgAAAAxs6aEw5M4AAAAAi0koAAAAwMiW5/Aj7gwAAACwmIQCAAAAjByxysOIhAIAAACwmIICAAAAsJgpDwAAADCwpSnjkDsDAAAALCahAAAAACOaMg5JKAAAAACLSSgAAADAiB4KQ+4MAAAAsJiEAgAAAIxs6aEwIqEAAAAALCahAAAAACNHPIcfcWcAAACAxSQUAAAAYMQqD0PuDAAAALCYhAIAAACMHLHKw4iEAgAAALCYhAIAAACM6KEw5M4AAAAAiykoAAAAAIuZ8gAAAAAjW5oyjkgoAAAAAItJKAAAAMDIEc/hR9wZAAAAYDEJBQAAABjRQ2FIQgEAAABYTEIBAAAARrY8hx9xZwAAAIDFJBQAAABgxCoPQ+4MAAAAsJiEAgAAAIxY5WFIQgEAAABYTEIBAAAARqzyMOTOAAAAAItJKAAAAMCIHgpDEgoAAADAYgoKAAAAwGKmPAAAAMDIEc/hR9wZAAAAYDEJBQAAABhYaco4JKEAAAAALCahAAAAACNbnsOPuDMAAADAYhIKAAAAMCKhMOTOAAAAAItJKAAAAMCAVR7GJBQAAACAxSQUAAAAYEQPhSF3BgAAAFhMQgEAAABG9FAYklAAAAAAFluSUPi66rxqVV1W/fmejAgAAAA2xRHP4UfWvTNPrS6tvq96XPXm6h/t1aAAAACAzbZuQeGfVfeunlw9qfr26p+f4PoLq8uryx/9yLNOZnwAAADAtR5Vva/6YPWs47z+5Oovqyvm7ak7XntS9YF5e9LJDmTdKQ9XVn+74/hvq4+d4PqL5q2LL/nE6roNDQAAAA7WarOaMp5Rvbh6RNPv6ZdVF1fvOea6V1bPOObcravnVvdtamXw1vm9n7qug1m3oPD/VW+pfmv+4PObpkA8c379X1/XAQAAAABrOa8pmfDh+fgVTb+fH1tQOJ5HVq+rrpqPX9eUdvi16zqYdQsKH5q3o35r/vPm1/WDAQAAYONtbVRTxrP7ytkCV1b3O851j62+s3p/9aPze4733rNPZjDrFhT+5Y79W1V/3ZRUAAAAAE6NC+ftqP/eTmB2vPkXx/5u/n83pQ6+WD2teln10DXfu8hupZbnVHef929UvaEpqfAX1cNP5oMBAABg0622juzb1lQ8uO+O7aJjhnNldacdx3esPn7MNX/VVEyo+sWmRRXWfe8iuxUULmjqHllTB8gj1e2qB1X/x8l8MAAAALDIZdU51TdUZ1ZPaGqsuNPOpRYfXf3pvH9J9V1Nsw5uNe9fcjKD2W3Kw5e6NgLxyKbYxDXzgNadLgEAAACH02at8nB10+oNlzSt+PDS6t3V86vLm4oLP9xUSLi6qQHjk+f3XlX9eFNRovk9Rxs0Xie7FQW+WH1L0xSHh1T/dMdrX3MyHwwAAAAs9tp52+k5O/afPW/H89J5OyV2Kyj8SPUbTdMcfqb6yHz+71RvP1WDAAAAgE202qxVHjbKbgWFt3RtU8adjlcRAQAAAE4T65ZablO9qHpb9dbq387nAAAA4Ppra2v/tkNm3YLCK6q/rB5bPW7ef+VeDQoAAADYbOuu1HDrpm6QR/2r6jGnfjgAAACwQfRQGFr3zryxaX3LI/P2+Op39mpQAAAAwGbbLaHwt9Wq2qqeWb18Pn9G9ZnquXs3NAAAADhYq0PY22C/7FZQuPm+jAIAAAA4VHYrKNy9em91n8Hrbzu1wwEAAAAOg90KCs+sLqx+ese51Y79h57yEQEAAMCm0JRxaLc780vV11UPmbdfbuqd8K6m5SMBAACA09BuBYV/X31p3v/O6ieql1Wfri7aw3EBAADAgVu1tW/bYbPblIczqqvm/Quaigivnrcr9nBcAAAAwAZbp6Bwg+rq6mFN/RTWfS8AAAAcais9FIZ2Kwr8WvUH1X+rPl/90Xz+m5qmPQAAAACnod0KCi+oXl+dVf1e167wcKT6J3s4LgAAADh4EgpD60xbePNxzr3/VA8EAAAAODz0QQAAAICB1dbhW31hv8huAAAAAItJKAAAAMCAVR7G3BkAAABgMQkFAAAAGNFDYUhCAQAAAFhMQgEAAAAG9FAYc2cAAACAxRQUAAAAgMVMeQAAAICBVZoyjkgoAAAAAItJKAAAAMCApoxj7gwAAACwmIQCAAAAjGzpoTAioQAAAAAsJqEAAAAAAyvP4YfcGQAAAGAxCQUAAAAYWOmhMCShAAAAACwmoQAAAAADqy3P4UfcGQAAAGAxCQUAAAAYWKWHwoiEAgAAALCYhAIAAAAM6KEw5s4AAAAAiykoAAAAAIuZ8gAAAAADqy1NGUckFAAAAIDFJBQAAABgwLKRYxIKAAAAwGISCgAAADBg2cgxdwYAAABYTEIBAAAABvRQGJNQAAAAABaTUAAAAIABPRTG3BkAAABgMQkFAAAAGNBDYUxCAQAAAFhMQgEAAAAG9FAYc2cAAACAxSQUAAAAYEAPhTEJBQAAAGCxPU8onH3uXff6I4DjOO8nX37QQ4DT0qXf9oMHPQQ4Ld3iircf9BDgNHb9Dr6vtiQURiQUAAAAgMUUFAAAAIDFrt/ZFAAAADgJq5UpDyMSCgAAAMBiEgoAAAAwsPIcfsidAQAAABaTUAAAAICBVXoojEgoAAAAAItJKAAAAMCAhMKYhAIAAACwmIQCAAAADEgojEkoAAAAAItJKAAAAMCAhMKYhAIAAACwmIQCAAAADKxWEgojEgoAAADAYhIKAAAAMKCHwpiEAgAAALCYggIAAACwmCkPAAAAMGDKw5iEAgAAALCYhAIAAAAMSCiMSSgAAAAAi0koAAAAwMBqJaEwIqEAAAAALCahAAAAAAPbeigMSSgAAAAAi0koAAAAwIBVHsYkFAAAAIDFJBQAAABgwCoPYxIKAAAAwGISCgAAADCgh8KYhAIAAACwmIQCAAAADOihMCahAAAAACymoAAAAACHx6Oq91UfrJ51nNefWb2nekf1+uouO167prpi3i4+2YGY8gAAAAADG9aU8YzqxdUjqiury5oKA+/Zcc3bq/tWn6v+t+qnqgvm1z5f3etUDUZCAQAAAA6H85qSCR+uvlS9ojr/mGve2FRMqHpzdce9GoyCAgAAAAysVlv7tq3h7OpjO46vnM+NPKX6LzuOb1xd3lRoeMzCW/FVTHkAAACAzXDhvB110bwddbyqw2rwd/2DpqkPD9px7s7Vx6tvrN5QvbP60HUdrIICAAAADGzv78cdW0A41pXVnXYc37GpQHCsh1f/oqmY8MUd549e++HqTdW9O4mCgikPAAAAcDhcVp1TfUN1ZvWEvnq1hntXv1A9uvrkjvO3qm4079+2emBf2cxxMQkFAAAAGFizt8F+ubp6RnVJ04oPL63eXT2/qTfCxdX/Wd2setX8no82FRfu0VRo2G4KF7wwBQUAAAA4bbx23nZ6zo79hw/e98fVt57KgSgoAAAAwMDquH0QKT0UAAAAgOtAQgEAAAAGNqyHwkaRUAAAAAAWk1AAAACAAT0UxiQUAAAAgMUkFAAAAGBge3XQI9hcEgoAAADAYgoKAAAAwGKmPAAAAMCApoxjEgoAAADAYhIKAAAAMLBaSSiMSCgAAAAAi0koAAAAwMDKspFDEgoAAADAYhIKAAAAMLBtlYchCQUAAABgMQkFAAAAGLDKw5iEAgAAALCYhAIAAAAMWOVhTEIBAAAAWGxJQuHs6i7HvOcPT+1wAAAAYHOsrPIwtG5B4SerC6r3VNfM51YpKAAAAMBpad2CwmOqc6svrnn9hfPWw+53s17/ls9ch6EBAADAwdrWQ2Fo3R4KH65uuODvvai6b3VfxQQAAAC4/tktofCzTVMbPlddUb2+r0wp/PAejQsAAADYYLsVFC6f/3xrdfEejwUAAAA2ymqlKePIbgWFl81/3rT6Qtc2ZDyjutFeDQoAAADYbOv2UHh9dZMdxzepfv/UDwcAAAA2x2q1f9ths25B4cbVzu6Kn6m+5tQPBwAAADgM1l028rPVfaq3zcffXn1+T0YEAAAAG2I7PRRG1i0o/Ej1qurj8/FZ1QV7MiIAAABg461TUDhSnVndvTq32qreW315D8cFAAAAB+4w9jbYL+v0UNiufrqpgPCu6p0pJgAAAMBpbd2mjL9XPbZMHgEAAOD0sVpt7dt22KzbQ+GZ1U2rq6svNBUWVtUt9mhcAAAAwAZbt6Bw8z0dBQAAAGygbT0UhtYtKFTdqjqnuvGOc394aocDAAAAHAbrFhSe2rR05B2rK6r7V39SPXSPxgUAAAAHzioPY+s2ZfyR6juqP6seUt27+su9GhQAAACw2dZNKHxh3qpuVL23OndPRgQAAAAbYmWxw6F1CwpXVresfrN6XfWp6uN7NSgAAABgs61bUPh785/Pq95YfW31u3sxIAAAAGDz7VZQuHH1tOqbqndWL6n+YK8HBQAAAJvAspFjuzVlfFl136ZiwndXP73nIwIAAAA23m4JhXtW3zrvv6S6dG+HAwAAAJvDspFjuyUUvrxj/+q9HAgAAABweOyWUPi26m/m/a3qJvPxVrWqbrF3QwMAAICDJaEwtltB4Yx9GQUAAABwqKy7bCQAAACcdrZXWwc9hI21Ww8FAAAAgK8ioQAAAAADeiiMSSgAAAAAi0koAAAAwICEwpiEAgAAALCYhAIAAAAMbEsoDEkoAAAAAItJKAAAAMDAarV10EPYWBIKAAAAwGIKCgAAAMBipjwAAADAgGUjxyQUAAAAgMUkFAAAAGDAspFjEgoAAADAYhIKAAAAMKCHwpiEAgAAALCYhAIAAAAMSCiMSSgAAAAAi0koAAAAwIBVHsYkFAAAAIDFJBQAAABgQA+FMQkFAAAAYDEJBQAAABjY3j7oEWwuCQUAAABgMQkFAAAAGNBDYUxCAQAAAFhMQQEAAABYzJQHAAAAGDDlYUxCAQAAAFhMQgEAAAAGtiUUhiQUAAAAgMUkFAAAAGBgta9NFLb28bNOnoQCAAAAsJiEAgAAAAxY5WFMQgEAAABYTEIBAAAABra3D3oEm0tCAQAAAFhMQgEAAAAG9FAYk1AAAAAAFpNQAAAAgIFtCYWhPS8ofM/33mWvPwI4jjd//uyDHgKclm5xxdsPeghwWvrMve590EOA09eX33fQI+CASCgAAADAgB4KY3ooAAAAAIspKAAAAACLmfIAAAAAA6t97cq4tY+fdfIkFAAAAODweFT1vuqD1bOO8/qNqlfOr7+luuuO1549n39f9ciTHYiEAgAAAAxs2LKRZ1Qvrh5RXVldVl1cvWfHNU+pPlV9U/WE6ierC6p7zsffXH199fvV3aprrutgJBQAAADgcDivKWHw4epL1Suq84+55vzqZfP+b1QPa5pLcf58/Rerj8x/z3knMxgFBQAAABhYrfZvW8PZ1cd2HF85nxtdc3X16eo2a753EQUFAAAA2AwXVpfv2C485vXjdW08thQxumad9y6ihwIAAAAMbO9vE4WL5m3kyupOO47vWH18cM2VTb/zf2111ZrvXURCAQAAAA6Hy6pzqm+ozmxqsnjxMddcXD1p3n9c9YamJMLF8/U3mt9/TnXpyQxGQgEAAAAG1uxtsF+urp5RXdK04sNLq3dXz2+aInFx9ZLq5U1NF69qKiI0X/frTStCXF09vZNY4aEUFAAAAOAwee287fScHftfqL5/8N4XzNspoaAAAAAAAxuWUNgoeigAAAAAi0koAAAAwMC2iMKQhAIAAACwmIQCAAAADKy2D3oEm0tCAQAAAFhMQQEAAABYzJQHAAAAGFhpyjgkoQAAAAAsJqEAAAAAA9uaMg5JKAAAAACLSSgAAADAgB4KYxIKAAAAwGISCgAAADCwLaAwJKEAAAAALCahAAAAAAMrEYUhCQUAAABgMQkFAAAAGLDIw5iEAgAAALCYhAIAAAAMbOuhMCShAAAAACwmoQAAAAADK00UhiQUAAAAgMUkFAAAAGBgtX3QI9hcEgoAAADAYgoKAAAAwGKmPAAAAMDAtqaMQxIKAAAAwGISCgAAADBg2cgxCQUAAABgMQkFAAAAGNjellAYkVAAAAAAFpNQAAAAgAEtFMYkFAAAAIDFJBQAAABgYKWHwpCEAgAAALCYhAIAAAAMbGuiMCShAAAAACwmoQAAAAADeiiMSSgAAAAAi0koAAAAwICEwpiEAgAAALCYggIAAACwmCkPAAAAMGDGw5iEAgAAALCYhAIAAAAMaMo4JqEAAAAALCahAAAAAAOrlYTCiIQCAAAAsJiEAgAAAAxs66EwJKEAAAAALCahAAAAAAN6KIxJKAAAAACLrZtQ2Kp+oPrG6vnVnauvqy7do3EBAADAgVvpoTC0bkLh56oHVE+cj/+2evGejAgAAADYeOsmFO5X3ad6+3z8qerME1x/4bx1zln1gU9c5/EBAADAgZFQGFu3oPDl6ozq6J28XbV9gusvmrc+8IncfQAAALieWbeg8KLqNdXtqxdUj6t+bK8GBQAAAJtg2yoPQ+sWFH61emv1sKYGjY+p/nSvBgUAAABstnUKCkeqd1TfUr13b4cDAAAAHAbrFBS2q//atFTkR/d2OAAAALA5NGUcW3fKw1nVu6tLq8/uOP/oUz4iAAAAYOOtW1D4l3s6CgAAANhAK00Zh9YtKPzBno4CAAAAOFTWLSjcv/rZ6h7VmdUZTVMfbrFH4wIAAIADt62HwtCRNa/7d9UTqw9UN6meOp8DAAAATkPrJhSqPtiUTLim+g/VH+/JiAAAAGBDWOVhbN2CwueapjpcUf1U9Ynqpns1KAAAAGCzrTvl4Qfna5/R1DvhTtVj92pQAAAAsAlWq9W+bYfNbgmFO1cfrf5sPv5ClpAEAACA095uCYXf3LH/6r0cCAAAAGya1fb2vm2HzW4Fha0d+9+4lwMBAAAADo/dpjysBvsAAABwvbdtlYeh3QoK31b9TVNS4SbzfvPxqrrF3g0NAAAA2FS7FRTO2JdRAAAAwAY6jKsv7Jd1l40EAAAA+O8UFAAAAIDFdpvyAAAAAKetlaaMQxIKAAAAwGISCgAAADAgoTAmoQAAAAAsJqEAAAAAA9ur7YMewsaSUAAAAAAWk1AAAACAAT0UxiQUAAAAgMUkFAAAAGBAQmFMQgEAAABYTEIBAAAABlYrCYURCQUAAABgMQkFAAAAGNje3j7oIWwsCQUAAABgMQkFAAAAGLDKw5iEAgAAALCYggIAAACwmCkPAAAAMLBaaco4IqEAAAAAh9+tq9dVH5j/vNVxrrlX9SfVu6t3VBfseO2Xq49UV8zbvXb7QAUFAAAAGFhtr/ZtO0nPql5fnTP/+azjXPO56oeqb64eVf2b6pY7Xv9nTYWEezUVFU5IQQEAAAAOv/Orl837L6sec5xr3t+UYKj6ePXJ6nbX9QMVFAAAAGDgECUU7lB9Yt7/RHX7Xa4/rzqz+tCOcy9omgrxM9WNdvtABQUAAADYDBdWl+/YLjzm9d+v3nWc7fyFn3NW9fLqH1ZHu04+u7p79R1N/Rj++W5/iVUeAAAAYGB7f1d5uGjeRh5+gtf+oqlQ8In5z08OrrtF9TvVj1Vv3nH+aLrhi9V/qP7pboOVUAAAAIDD7+LqSfP+k6rfOs41Z1avqX6letUxr501/7nV1H/hXbt9oIQCAAAADJyC3gb75YXVr1dPqT5aff98/r7V06rzIfCIAAAGM0lEQVSnVo+vvrO6TfXk+fUnN63o8KtNDRq35uOn7faBCgoAAABw+P1V9bDjnL+8qZhQ9R/n7XgeuvQDFRQAAABgYLW9rz0UDhU9FAAAAIDFJBQAAABg4BD1UNh3EgoAAADAYhIKAAAAMLBa6aEwIqEAAAAALKagAAAAACxmygMAAAAMbGvKOCShAAAAACwmoQAAAAADq21NGUckFAAAAIDFJBQAAABgYKWHwpCEAgAAALCYhAIAAAAMrFZ6KIxIKAAAAACLSSgAAADAgB4KYxIKAAAAwGISCgAAADCw2tZDYURCAQAAAFhsa7UyH4QTurC66KAHAach3z04GL57cDB89+AQklBgNxce9ADgNOW7BwfDdw8Ohu8eHEIKCgAAAMBiCgoAAADAYgoK7MZcNjgYvntwMHz34GD47sEhpCkjAAAAsJiEAgAAALCYggJ/r1pVdz/ogcD13L+o3l29o7qiul/1S9U959c/M3jf/au3zO/50+p5ezpKuH65pum7867qVdXXnIK/88nVvzsFfw+cLo5+D49udz3Q0QCn1A0OegAcuCdW/0/1hPyiAnvlAdXfre5TfbG6bXVm9dQ13vuy6vHVf63OqM7dozHC9dHnq3vN+79aPa3612u+94ymX4SAk7Pze7iE7yAcAhIKp7ebVQ+sntJUUKjp/xM/1/Qk9ber11aPm1/79uoPqrdWl1Rn7edg4RA7q/pvTcWE5v2PV2+q7rvjup+u3la9vrrdfO721Sfm/Wuq98z7z6teXr2h+kD1v+7JyOH644+qb5r3f7PpZ9m7qwt3XPOZ6vlNqaAHVN9R/XFTQe/S6ubzdV9f/W7Td++n9nrgcD1016bv5Nvm7X+azz+4emP1n6p3zuf+QdP374rqF5oKDcCGUFA4vT2m6R9E76+uanp6+n1N/5H/1qanpw+Yr71h9bNNxYVvr15avWB/hwuH1u9Vd2r6rv1c9aDjXHPTpn9U3aepcPfc+fzPVO+rXlP94+rGO97zP1bf0/Q9fU7TLznAV7tB9d1d+wvKP2r6WXbf6oer28znb9o0PeJ+Tb/AvLL6kerbqoc3PWmt6WnrBU0/Ky9o+n4Dx3eTrp3u8Jr53CerRzT9zLugetGO689rmiZ4z+oe8+sPbPreXVP9wL6MGliLKQ+ntydW/2bef8V8fMOmeabb1Z83VYlrill/S/W6+fiMrn1qCpzYZ5p+eflfqoc0/ZLyrGOu2Z7PV/3H6j/P+89vimp/V/X3m76nD55f+62mX3A+3/RdPa/pySswOfqLTE1PQ18y7/9wUw+hmooB51R/1fTLyqvn8+c2/Zy7bD7+mx1/7+urT8/776nuUn3sFI8dri+ON+Xhhk29SI4WCe6247VLq4/M+w9r+vl59Ht4k6ZiBLAhFBROX7epHtpUJFg1FQhWXVs5PtZWUzT0AYPXgRO7pmmKw5uanpI+aZfrd67p+6Hq56tfrP6ya5+mHrvur3WA4Ssd7xeZBzelDR5Qfa7pO3k0+fOFrp2zvdX4O/XFHfvX5N9TsNSPVn/RlP450vTdO+qzO/a3mnoJPXv/hgYsYcrD6etx1a80PVW5a9MTmo80ze1+bNP/N+7QtU9C39c0p3vnFIhv3rfRwuF2btMT0KPuVf3ZMdcc6dp+JX+/qVlqTVMatub9c5p+efnr+fj8pl+EbtP0XT36BAcY+9rqU03FhLs3raRyPO9tmkb0HfPxzVM4gFPla5sSQNvVDzbui/D6pp+Nt5+Pb930b1dgQ/jBePp6YvXCY869ummu2pVNc0jf39SY6tPVl5r+g/6iph8CN2iaLvHufRovHGY3a+pBcsvq6uqDTY3gfmPHNZ9tKtK9tek7d8F8/geb+ih8bn7vD3TtE9RLq9+p7lz9eFOjR+DEfrdptYd3NBXL3zy47ktN38OfbYpZf74p2QCcvJ9r+nfn9zdN2fvs4Lr3VD/W1IvoSPXl6ul9dVEeOCBbq5WELF/lZk1zvm/T9AvLA5v6KQCb43lN39P/64DHAQDAaUpCgeP57aYnqWc2PfVUTAAAAOArSCgAAAAAi2nKCAAAACymoAAAAAAspqAAAAAALKagAAAAACymoAAAAAAspqAAAAAALPb/Aw3hMXTromvyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(titanic_df[numeric].corr(), cmap = \"coolwarm\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id = \"code\"></a>\n",
    " ### Coding Logistic Regression\n",
    "\n",
    " The first function performs the preprocessing of our data. The steps are:\n",
    "\n",
    " **First**: Ensure that the x- and y- matricies have the observations as rows, and features as columns.\n",
    " - The x-matrix will be n-rows and d-columns. Where $n>d$\n",
    " - The y-vector will be a 1-dimensional numpy array of length n.\n",
    "\n",
    " **Second**: A column of ones is added to the x-inputs matrix, increasing its dimensions to n-by-d+1.\n",
    "\n",
    " **Third**: Ensure that the y-vector has all values encoded as 1 and -1, NOT 1 and 0.\n",
    "\n",
    " **Fourth**: The initial vector of weights is created, a vector of length d+1 of all 0's\n",
    "\n",
    " Those three matricies are all returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data(input_x, target_y):\n",
    "    \"\"\"\n",
    "    Confirm dimensions of x and y, transpose if appropriate;\n",
    "    Add column of ones to x;\n",
    "    Ensure y consists of 1's and -1's;\n",
    "    Create weights array of all 0s\n",
    "    \n",
    "    Return X, y, and weights.\n",
    "    \n",
    "    Arguments:\n",
    "        input_x - a numpy array \n",
    "        target_y - a numpy array\n",
    "        \n",
    "    Returns:\n",
    "        prepared_x -- a 2-d numpy array; first column consists of 1's,\n",
    "            more rows than columns\n",
    "        prepared_y -- a numpy array consisting only of 1s and -1s\n",
    "        initial_w -- a 1-d numpy array consisting of \"d+1\" 0s, where\n",
    "            \"d+1\" is the number of columns in \"prepared_x\"  \n",
    "    \n",
    "    Assumptions:\n",
    "        Assume that there are more observations than features in `input_x`\n",
    "    \"\"\"\n",
    "\n",
    "    if input_x.shape[0] < input_x.shape[1]:\n",
    "        input_x = np.transpose(input_x)\n",
    "        \n",
    "    ones = np.ones((len(target_y), 1), dtype=int)\n",
    "    \n",
    "    prepared_x = np.concatenate((ones, input_x), axis=1)\n",
    "    \n",
    "    #Convert (1,0) to (1,-1), doesn't catch exceptions\n",
    "    prepared_y = (target_y - 0.5)*2\n",
    "    \n",
    "    initial_w = np.zeros(prepared_x.shape[1], dtype=int)\n",
    "        \n",
    "    return prepared_x, prepared_y.astype(int), initial_w\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The next function calculates the value of the sigmoid.\n",
    "\n",
    " Equation for the sigmoid:\n",
    " $$\\sigma_i(y_i \\cdot w) = \\frac{e^{y_iX_i^Tw}}{1+e^{y_ix_i^Tw}}$$\n",
    "\n",
    " We define a function called `sigmoid_single`:\n",
    "\n",
    " **Given** $x_i$, $y_i$, and $w$\n",
    " **Return** a float, between 0 and 1.\n",
    "\n",
    " $e^{y_ix_i^Tw}$ will evaluate to $np.inf$ when $y_ix_i^Tw$ is greater than ~709.782. In this case, a `\"1\"` should be returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid_single(x, y, w):\n",
    "    \"\"\"\n",
    "    Obtain the value of a Sigmoid using training data.\n",
    "    \n",
    "    Arguments:\n",
    "        x - a vector of length d\n",
    "        y - either 1, or -1\n",
    "        w - a vector of length d\n",
    "    \"\"\"\n",
    "    \n",
    "    Sig_numer  = np.exp(y * np.matmul(np.transpose(x),w))\n",
    "    \n",
    "    if np.isinf(Sig_numer):\n",
    "        return 1\n",
    "    else:\n",
    "        return (Sig_numer / (1 + Sig_numer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " With the sigmoid, $\\sigma_i(y_i \\cdot w)$ defined above, we tackle the rest of the function that is summed to calculate the gradient of the log-likelihood.\n",
    "\n",
    " In a function named `to_sum`:\n",
    "\n",
    "\n",
    " **Given** $x_i$, $y_i$, and $w$\n",
    " **Return** $(1-\\sigma_i(y_i\\cdot w))y_ix_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_sum(x,y,w):\n",
    "    \"\"\"\n",
    "    Obtain the value of the function that will eventually be summed to \n",
    "    find the gradient of the log-likelihood.\n",
    "    \n",
    "    Arguments:\n",
    "        x - a vector of length d\n",
    "        y - either 1, or -1\n",
    "        w - a vector of length d\n",
    "    \"\"\"\n",
    "    \n",
    "    return (1 - sigmoid_single(x,y,w))*y*x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Finally, define a function called `sum_all`:\n",
    "\n",
    " **Given**: The pre-processed matricies corresponding to X, y, and weights\n",
    " **Return**: $\\sum_{i = 1}^n (1 − \\sigma_i(y_i \\cdot w))\\ y_i x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sum_all(x_input, y_target, w):\n",
    "    \"\"\"\n",
    "    Obtain and return the gradient of the log-likelihood\n",
    "    \n",
    "    Arguments:\n",
    "        x_input - *preprocessed* an array of shape n-by-d\n",
    "        y_target - *preprocessed* a vector of length n\n",
    "        w - a vector of length d\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    total = 0\n",
    "    for i in range(0, y_target.shape[0]):\n",
    "        total += to_sum(x_input[i,],y_target[i],w)\n",
    "    \n",
    "    return total\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Define a function called `update_w`, that performs a single-step of gradient descent for calculating the Logistic Regression weights:\n",
    "\n",
    " **Given**: Pre-processed matricies of x, and y; the current weights - $w_i$, and \"$\\eta$\"\n",
    " **Return**: $w_{i+1}$ Which is equal to: $w_i + \\eta \\sum_{i = 1}^n (1 − \\sigma_i(y_i \\cdot w_i))\\ y_i x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_w(x_input, y_target, w, eta):\n",
    "    \"\"\"Obtain and return updated Logistic Regression weights\n",
    "    \n",
    "    Arguments:\n",
    "        x_input - *preprocessed* an array of shape n-by-d\n",
    "        y_target - *preprocessed* a vector of length n\n",
    "        w - a vector of length d\n",
    "        eta - a float, positive, close to 0\n",
    "    \"\"\"   \n",
    "    return w + eta*sum_all(x_input,y_target,w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next, we create a function called `fixed_iteration` which performs gradient descent, calculating Logistic Regression weights for a specified number of steps.\n",
    "\n",
    "\n",
    " **Given**: *Un-preprocessed* x- and y- matricies, an $\\eta$ parameter that is positive and close to 0, and an integer number of steps\n",
    " **Return**: $w_{steps}$ where $w_{i+1} = w_i + \\eta \\sum_{i = 1}^n (1 − \\sigma_i(y_i \\cdot w_i))\\ y_i x_i$\n",
    "\n",
    " NB: Initial weights ($w_0$) should all be 0's like are returned from the `prepare_data` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fixed_iteration(x_input, y_target, eta, steps):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return weights calculated from 'steps' number of steps of gradient descent.\n",
    "    \n",
    "    Arguments:\n",
    "        x_input - *NOT-preprocessed* an array\n",
    "        y_target - *NOT-preprocessed* a vector of length n\n",
    "        eta - a float, positve, close to 0\n",
    "        steps - an int    \n",
    "    \"\"\" \n",
    "\n",
    "    x,y,w = prepare_data(x_input,y_target)\n",
    "    \n",
    "    #print(w)\n",
    "    for i in range(0,steps):\n",
    "        w = update_w(x, y, w, eta)\n",
    "    \n",
    "    return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For the final function, we create a prediction for out-of-sample data.\n",
    "\n",
    " Define a function called \"predict\".\n",
    " Accept two inputs:\n",
    " - An **un-preprocessed** observation of X -- a vector as numpy array\n",
    " - A numpy array of weights\n",
    "\n",
    " Return:\n",
    " A label prediction for the x observations; either -1 or 1 (integers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(x_input, weights):\n",
    "    \"\"\"\n",
    "    Return the label prediction, 1 or -1 (an integer), for the given x_input and LR weights.\n",
    "    \n",
    "    Arguments:\n",
    "        x_input - *NOT-preprocessed* a vector of length d-1\n",
    "        weights - a vector of length d\n",
    "  \n",
    "    \"\"\"\n",
    "    \n",
    "    prepared_x = np.concatenate(([1], x_input), axis=0)\n",
    "    \n",
    "    predicts = np.matmul(np.transpose(prepared_x),weights)\n",
    "    \n",
    "    return int(predicts / abs(predicts))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id = \"sklearn\"></a>\n",
    " ### Logistic Regression in `sklearn`\n",
    "\n",
    " The following cells demonstrate Logistic Regression using `sklearn`, and compare the custom Logistic Regression build in the previous functions to `sklearn's`\n",
    "\n",
    " [Logistic Regression in `sklearn` - Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.07454295]\n",
      "[[-0.88163975 -0.03085442 -0.29522779 -0.0771424   0.00431432 -2.40900801\n",
      "   0.08754859 -0.21682785]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaseidel\\.conda\\envs\\home37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(titanic_imputed, y_target)\n",
    "\n",
    "# Create sklearn's predictions\n",
    "sk_pred = lr.predict(titanic_imputed)\n",
    "\n",
    "print(lr.intercept_)\n",
    "print(lr.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " While the particular coeffcients will be very different (regularization is implemented in the `sklearn` instantiation), at least the signs should mostly be the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6853.75182849   -831.23291018   -304.24383135  -2919.42190116\n",
      "  -1291.68658091    218.83793151 -14297.98042097     66.49415552\n",
      "    428.14909561]\n"
     ]
    }
   ],
   "source": [
    "# This cell may take awhile\n",
    "wt = fixed_iteration(titanic_imputed.values, y_target.values, .05, 12000)\n",
    "\n",
    "print(wt)\n",
    "\n",
    "cust_preds = np.array([predict(x,wt) for x in titanic_imputed.values])\n",
    "cust_preds[cust_preds == -1] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85       549\n",
      "           1       0.77      0.69      0.73       340\n",
      "\n",
      "    accuracy                           0.80       889\n",
      "   macro avg       0.80      0.78      0.79       889\n",
      "weighted avg       0.80      0.80      0.80       889\n",
      "\n",
      "Custom:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.94      0.82       549\n",
      "           1       0.82      0.41      0.54       340\n",
      "\n",
      "    accuracy                           0.74       889\n",
      "   macro avg       0.77      0.67      0.68       889\n",
      "weighted avg       0.76      0.74      0.71       889\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"sklearn:\")\n",
    "print(classification_report(y_target, sk_pred))\n",
    "\n",
    "print(\"Custom:\")\n",
    "print(classification_report(y_target, cust_preds))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
